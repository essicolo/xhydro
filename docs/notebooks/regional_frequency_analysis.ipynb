{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xclim\n",
    "import xdatasets as xd\n",
    "from scipy import stats\n",
    "from sklearn.cluster import HDBSCAN, OPTICS, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import xhydro as xh\n",
    "import xhydro.frequency_analysis as xhfa\n",
    "import xhydro.gis as xhgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Document and explain\n",
    "# TODO Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmoments3.distr import KappaGen\n",
    "\n",
    "from xhydro.frequency_analysis.distr import KappaGen\n",
    "from xhydro.frequency_analysis.regional import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start with getting the 02 region stations that are natural and have a minimum duration of 15 years\n",
    "ds = (\n",
    "    xd.Query(\n",
    "        **{\n",
    "            \"datasets\": {\n",
    "                \"deh\": {\n",
    "                    \"id\": [\"02*\"],\n",
    "                    \"regulated\": [\"Natural\"],\n",
    "                    \"variables\": [\"streamflow\"],\n",
    "                }\n",
    "            },\n",
    "            \"time\": {\"start\": \"1970-01-01\", \"minimum_duration\": (15 * 365, \"d\")},\n",
    "        }\n",
    "    )\n",
    "    .data.squeeze()\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# This dataset lacks some of the aforementioned attributes, so we need to add them.\n",
    "ds[\"id\"].attrs[\"cf_role\"] = \"timeseries_id\"\n",
    "ds[\"streamflow\"].attrs = {\n",
    "    \"long_name\": \"Streamflow\",\n",
    "    \"units\": \"m3 s-1\",\n",
    "    \"standard_name\": \"water_volume_transport_in_river_channel\",\n",
    "    \"cell_methods\": \"time: mean\",\n",
    "}\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we hide years with more than 15% of missing data and get yearly max and spring max\n",
    "\n",
    "timeargs = {\n",
    "    \"spring\": {\"date_bounds\": [\"02-11\", \"06-19\"]},\n",
    "    \"annual\": {},\n",
    "}\n",
    "\n",
    "ds_4fa = xh.indicators.get_yearly_op(\n",
    "    ds, op=\"max\", timeargs=timeargs, missing=\"pct\", missing_options={\"tolerance\": 0.15}\n",
    ")\n",
    "\n",
    "ds_4fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a daily volume from a daily streamflow\n",
    "ds[\"volume\"] = xh.indicators.compute_volume(ds[\"streamflow\"], out_units=\"hm3\")\n",
    "\n",
    "# We'll take slightly different indexers\n",
    "timeargs_vol = {\"spring\": {\"date_bounds\": [\"04-30\", \"06-15\"]}, \"annual\": {}}\n",
    "\n",
    "# The operation that we want here is the sum, not the max.\n",
    "ds_4fa = xr.merge(\n",
    "    [\n",
    "        ds_4fa,\n",
    "        xh.indicators.get_yearly_op(\n",
    "            ds,\n",
    "            op=\"sum\",\n",
    "            input_var=\"volume\",\n",
    "            timeargs=timeargs_vol,\n",
    "            missing=\"pct\",\n",
    "            missing_options={\"tolerance\": 0.15},\n",
    "            interpolate_na=True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "ds_4fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets get the shapesfile of those stations catchments\n",
    "gdf = xd.Query(\n",
    "    **{\n",
    "        \"datasets\": {\n",
    "            \"deh_polygons\": {\n",
    "                \"id\": [\"02*\"],\n",
    "                \"regulated\": [\"Natural\"],\n",
    "                \"variables\": [\"streamflow\"],\n",
    "            }\n",
    "        },\n",
    "        \"time\": {\"start\": \"1970-01-01\", \"minimum_duration\": (15 * 365, \"d\")},\n",
    "    }\n",
    ").data.reset_index()\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With those catchments, we can now calculate some of the catchments properties.\n",
    "# We could also get meteorological values and land use data. Refer to GIS example for more details.\n",
    "dswp = xhgis.watershed_properties(\n",
    "    gdf[[\"Station\", \"geometry\"]], unique_id=\"Station\", output_format=\"xarray\"\n",
    ")\n",
    "cent = dswp[\"centroid\"].to_numpy()\n",
    "lon = [ele[0] for ele in cent]\n",
    "lat = [ele[1] for ele in cent]\n",
    "dswp = dswp.assign(lon=(\"Station\", lon))\n",
    "dswp = dswp.assign(lat=(\"Station\", lat))\n",
    "dswp = dswp.drop(\"centroid\")\n",
    "dswp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do our regional frequency analysis, we'll process the data with a principal component analysis (PCA)\n",
    "data, pca = xhfa.regional.fit_pca(dswp, n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the correlation is close to 0 between the components\n",
    "data.to_dataframe(name=\"value\").reset_index().pivot(\n",
    "    index=\"Station\", columns=\"components\"\n",
    ").corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different clustering methods can be used. Their parameters can be passed as a dict.\n",
    "# We're using AgglomerativeClustering here but later we'll show and exammple using a combination of different methods.\n",
    "xhfa.regional.get_group_from_fit(HDBSCAN, {\"min_cluster_size\": 2}, data)\n",
    "xhfa.regional.get_group_from_fit(OPTICS, {\"min_samples\": 2}, data)\n",
    "groups = xhfa.regional.get_group_from_fit(\n",
    "    AgglomerativeClustering, {\"n_clusters\": 3}, data\n",
    ")\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the L-moments for each station\n",
    "ds_moment = xr.apply_ufunc(\n",
    "    moment_l_vector, ds_4fa, input_core_dims=[[\"time\"]], output_core_dims=[[\"lmom\"]]\n",
    ").assign_coords(lmom=[\"l1\", \"l2\", \"l3\", \"tau\", \"tau3\", \"tau4\"])\n",
    "ds_moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We then create groupes of values and moments for each group.\n",
    "ds_groups = xr.concat(\n",
    "    [\n",
    "        ds_4fa.sel(id=groups[i]).assign_coords(group_id=i).expand_dims(\"group_id\")\n",
    "        for i in range(len(groups))\n",
    "    ],\n",
    "    dim=\"group_id\",\n",
    ")\n",
    "ds_moments_groups = xr.concat(\n",
    "    [\n",
    "        ds_moment.sel(id=groups[i]).assign_coords(group_id=i).expand_dims(\"group_id\")\n",
    "        for i in range(len(groups))\n",
    "    ],\n",
    "    dim=\"group_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each group, calculate the H and Z values\n",
    "kap = KappaGen()\n",
    "ds_H_Z = calc_h_z(ds_groups, ds_moments_groups, kap)\n",
    "ds_H_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter the data to only include the data that has H and Z below the thresholds\n",
    "mask = mask_h_z(ds_H_Z)\n",
    "ds_groups_H1 = ds_groups.where(mask).load()\n",
    "ds_moments_groups_H1 = ds_moments_groups.where(mask).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centiles are return periods :\n",
    "centiles = [x / 100.0 for x in range(101)]\n",
    "return_periods = [\n",
    "    1.010101,\n",
    "    1.052632,\n",
    "    1.111111,\n",
    "    1.25,\n",
    "    1.5,\n",
    "    2,\n",
    "    3,\n",
    "    5,\n",
    "    10,\n",
    "    20,\n",
    "    50,\n",
    "    100,\n",
    "    200,\n",
    "    500,\n",
    "    1000,\n",
    "    2000,\n",
    "    5000,\n",
    "    10000,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now calculate the values for each group and return period and we remove the regions with less stations than a threshold\n",
    "Q_T = calculate_rp_from_afr(ds_groups_H1, ds_moments_groups_H1, return_periods)\n",
    "Q_T = remove_small_regions(Q_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot, let see what it looks like on 023401\n",
    "Q_reg = Q_T.sel(id=\"023401\").dropna(dim=\"group_id\", how=\"all\")\n",
    "reg = Q_reg.streamflow_max_annual.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Compare local and regional\n",
    "params_loc = xhfa.local.fit(ds_4fa)\n",
    "Q_loc = xhfa.local.parametric_quantiles(params_loc, return_periods)\n",
    "loc = Q_loc.sel(id=\"023401\", scipy_dist=\"genextreme\").streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Q_reg\n",
    "plt.plot(reg.rp.values, reg.values, \"blue\")\n",
    "plt.plot(loc.return_period.values, loc.values, \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some uncertainities\n",
    "But we will work with only one catchemnt and two distributions as uncertinities can be intensive in computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We selct the station 023401, and distribution 'genextreme' and 'pearson3'\n",
    "ds_4fa_one_station = ds_4fa.sel(id=\"023401\")\n",
    "params_loc_one_station = params_loc.sel(\n",
    "    id=\"023401\", scipy_dist=[\"genextreme\", \"pearson3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We bootstrap the observations 200 times to get the uncertainty\n",
    "ds_4fa_iter = xhfa.uncertainities.boostrap_obs(ds_4fa_one_station, 200)\n",
    "params_boot_obs = xhfa.local.fit(ds_4fa_iter, distributions=[\"genextreme\", \"pearson3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the values\n",
    "Q_boot_obs = xhfa.local.parametric_quantiles(\n",
    "    params_boot_obs.load(), return_periods\n",
    ").squeeze()\n",
    "Q_boot_obs = Q_boot_obs.streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ressample the fittted distributions 200 times to get the uncertainty\n",
    "values = xhfa.uncertainities.boostrap_dist(\n",
    "    ds_4fa_one_station, params_loc_one_station, 200\n",
    ")\n",
    "params_boot_dist = xhfa.uncertainities.fit_boot_dist(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the values\n",
    "Q_boot_dist = xhfa.local.parametric_quantiles(\n",
    "    params_boot_dist.load(), return_periods\n",
    ").squeeze()\n",
    "Q_boot_dist = Q_boot_dist.streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dist = Q_boot_dist.sel(scipy_dist=\"genextreme\")\n",
    "loc_obs = Q_boot_obs.sel(scipy_dist=\"genextreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(reg.rp.values, reg.values, \"blue\", label=\"Regional\")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.5, \"samples\"),\n",
    "    \"red\",\n",
    "    label=\"bootstrap obs\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.05, \"samples\"),\n",
    "    \"pink\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_obs.return_period.values, loc_obs.quantile(0.95, \"samples\"), \"pink\")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.5, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"bootstrap dist\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.05, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_dist.return_period.values, loc_dist.quantile(0.95, \"samples\"), \"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regional, we need to ressample al stations, but this time, it's much faster as no fit is involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_reg_samples = xhfa.uncertainities.boostrap_obs(ds_4fa, 200)\n",
    "ds_moments_iter = xhfa.uncertainities.calc_moments_iter(ds_reg_samples).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_reg_boot = xhfa.uncertainities.calc_q_iter(\n",
    "    \"023401\", \"streamflow_max_annual\", ds_groups_H1, ds_moments_iter, return_periods\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_boot = Q_reg_boot.streamflow_max_annual.sel(id=\"023401\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(visible=True)\n",
    "plt.plot(\n",
    "    reg_boot.rp.values, reg_boot.quantile(0.5, \"samples\"), \"blue\", label=\"Regional\"\n",
    ")\n",
    "plt.plot(reg_boot.rp.values, reg_boot.quantile(0.05, \"samples\"), \"cyan\", label=\"95% CI\")\n",
    "plt.plot(reg_boot.rp.values, reg_boot.quantile(0.95, \"samples\"), \"cyan\")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.5, \"samples\"),\n",
    "    \"red\",\n",
    "    label=\"bootstrap obs\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.05, \"samples\"),\n",
    "    \"pink\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_obs.return_period.values, loc_obs.quantile(0.95, \"samples\"), \"pink\")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.5, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"bootstrap dist\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.05, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_dist.return_period.values, loc_dist.quantile(0.95, \"samples\"), \"green\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use regional uncertainties, first we use a jacknife to create more potential regions\n",
    "combinations_list = xhfa.uncertainities.generate_combinations(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second we try different clustering methods\n",
    "# We dont do too many tests here since it can take quite a while to run and the goal is just to illustrate the possibilities\n",
    "PARAM = {\n",
    "    AgglomerativeClustering: {\"arg_name\": \"n_clusters\", \"range\": range(2, 12)},\n",
    "    HDBSCAN: {\"arg_name\": \"min_cluster_size\", \"range\": range(6, 7)},\n",
    "    OPTICS: {\"arg_name\": \"min_samples\", \"range\": range(4, 5)},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So our station instead of beein gin one region, will be in many of the regions\n",
    "groups = []\n",
    "\n",
    "for model in [AgglomerativeClustering, HDBSCAN, OPTICS]:\n",
    "\n",
    "    for p in PARAM[model][\"range\"]:\n",
    "        d_param = {}\n",
    "        d_param[PARAM[model][\"arg_name\"]] = p\n",
    "        for combination in combinations_list:\n",
    "            # Extract data for the current combination\n",
    "            data_com = data.sel(Station=list(combination))\n",
    "            # Get groups from the fit and add to the list\n",
    "            groups = groups + get_group_from_fit(model, d_param, data_com)\n",
    "unique_groups = [list(x) for x in {tuple(x) for x in groups}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do as before\n",
    "ds_groups = xr.concat(\n",
    "    [\n",
    "        ds_4fa.sel(id=unique_groups[i])\n",
    "        .assign_coords(group_id=i)\n",
    "        .expand_dims(\"group_id\")\n",
    "        for i in range(len(unique_groups))\n",
    "    ],\n",
    "    dim=\"group_id\",\n",
    ")\n",
    "ds_moments_groups = xr.concat(\n",
    "    [\n",
    "        ds_moment.sel(id=unique_groups[i])\n",
    "        .assign_coords(group_id=i)\n",
    "        .expand_dims(\"group_id\")\n",
    "        for i in range(len(unique_groups))\n",
    "    ],\n",
    "    dim=\"group_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kap = KappaGen()\n",
    "ds_H_Z = calc_h_z(ds_groups, ds_moments_groups, kap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask_h_z(ds_H_Z)\n",
    "ds_groups_H1 = ds_groups.where(mask).load()\n",
    "ds_moments_groups_H1 = ds_moments_groups.where(mask).load()\n",
    "\n",
    "Q_T = calculate_rp_from_afr(ds_groups_H1, ds_moments_groups_H1, return_periods)\n",
    "Q_T = remove_small_regions(Q_T)\n",
    "\n",
    "Q = Q_T.sel(id=\"023401\").dropna(dim=\"group_id\", how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.plot(\n",
    "    regional_multiple_region.rp.values,\n",
    "    regional_multiple_region.quantile(0.5, \"group_id\"),\n",
    "    \"blue\",\n",
    "    label=\"regional_multiple_region\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region.rp.values,\n",
    "    regional_multiple_region.quantile(0.05, \"group_id\"),\n",
    "    \"cyan\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region.rp.values,\n",
    "    regional_multiple_region.quantile(0.95, \"group_id\"),\n",
    "    \"cyan\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.5, \"samples\"),\n",
    "    \"red\",\n",
    "    label=\"bootstrap obs\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.05, \"samples\"),\n",
    "    \"pink\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_obs.return_period.values, loc_obs.quantile(0.95, \"samples\"), \"pink\")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.5, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"bootstrap dist\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.05, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_dist.return_period.values, loc_dist.quantile(0.95, \"samples\"), \"green\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(visible=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also combine multiple regions and ressampling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_q_iter will check in how many group_id the station is present, and stack it with samples\n",
    "#  In this case, it will be stacked with 200 samples, and it's in 533 groupes so 103600 samples are generated.\n",
    "Q_reg_boot = xhfa.uncertainities.calc_q_iter(\n",
    "    \"023401\", \"streamflow_max_annual\", ds_groups_H1, ds_moments_iter, return_periods\n",
    ")\n",
    "Q_reg_boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_multiple_region_boot = Q_reg_boot.sel(id=\"023401\").streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.5, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"bootstrap dist\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.05, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_dist.return_period.values, loc_dist.quantile(0.95, \"samples\"), \"green\")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.5, \"samples\"),\n",
    "    \"red\",\n",
    "    label=\"bootstrap obs\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.05, \"samples\"),\n",
    "    \"pink\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_obs.return_period.values, loc_obs.quantile(0.95, \"samples\"), \"pink\")\n",
    "plt.plot(\n",
    "    regional_multiple_region_boot.rp.values,\n",
    "    regional_multiple_region_boot.quantile(0.5, \"samples\"),\n",
    "    \"black\",\n",
    "    label=\"regional multiple regions and boot\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region_boot.rp.values,\n",
    "    regional_multiple_region_boot.quantile(0.05, \"samples\"),\n",
    "    \"grey\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region_boot.rp.values,\n",
    "    regional_multiple_region_boot.quantile(0.95, \"samples\"),\n",
    "    \"grey\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region.rp.values,\n",
    "    regional_multiple_region.quantile(0.5, \"group_id\"),\n",
    "    \"blue\",\n",
    "    label=\"regional multiple regions\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region.rp.values,\n",
    "    regional_multiple_region.quantile(0.05, \"group_id\"),\n",
    "    \"cyan\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region.rp.values,\n",
    "    regional_multiple_region.quantile(0.95, \"group_id\"),\n",
    "    \"cyan\",\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(visible=True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
