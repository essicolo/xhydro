{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency analysis module - Regional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xclim\n",
    "import xdatasets as xd\n",
    "from lmoments3.distr import KappaGen\n",
    "from scipy import stats\n",
    "from sklearn.cluster import HDBSCAN, OPTICS, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import xhydro as xh\n",
    "import xhydro.frequency_analysis as xhfa\n",
    "import xhydro.gis as xhgis\n",
    "from xhydro.frequency_analysis.regional import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to use the xhydro package to perform regional frequency analysis on a dataset of streamflow data. The first steps will be similar to the local frequency analysis notebook, but will will keep it simple to focus on the regional frequency analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with getting the 02 region stations that are natural and have a minimum duration of 15 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    xd.Query(\n",
    "        **{\n",
    "            \"datasets\": {\n",
    "                \"deh\": {\n",
    "                    \"id\": [\"02*\"],\n",
    "                    \"regulated\": [\"Natural\"],\n",
    "                    \"variables\": [\"streamflow\"],\n",
    "                }\n",
    "            },\n",
    "            \"time\": {\"start\": \"1970-01-01\", \"minimum_duration\": (15 * 365, \"d\")},\n",
    "        }\n",
    "    )\n",
    "    .data.squeeze()\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# This dataset lacks some of the aforementioned attributes, so we need to add them.\n",
    "ds[\"id\"].attrs[\"cf_role\"] = \"timeseries_id\"\n",
    "ds[\"streamflow\"].attrs = {\n",
    "    \"long_name\": \"Streamflow\",\n",
    "    \"units\": \"m3 s-1\",\n",
    "    \"standard_name\": \"water_volume_transport_in_river_channel\",\n",
    "    \"cell_methods\": \"time: mean\",\n",
    "}\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we hide years with more than 15% of missing data and get yearly max and spring max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeargs = {\n",
    "    \"spring\": {\"date_bounds\": [\"02-11\", \"06-19\"]},\n",
    "    \"annual\": {},\n",
    "}\n",
    "\n",
    "ds_4fa = xh.indicators.get_yearly_op(\n",
    "    ds, op=\"max\", timeargs=timeargs, missing=\"pct\", missing_options={\"tolerance\": 0.15}\n",
    ")\n",
    "\n",
    "ds_4fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the following operations are simillar to the ones performed in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"volume\"] = xh.indicators.compute_volume(ds[\"streamflow\"], out_units=\"hm3\")\n",
    "\n",
    "timeargs_vol = {\"spring\": {\"date_bounds\": [\"04-30\", \"06-15\"]}, \"annual\": {}}\n",
    "\n",
    "ds_4fa = xr.merge(\n",
    "    [\n",
    "        ds_4fa,\n",
    "        xh.indicators.get_yearly_op(\n",
    "            ds,\n",
    "            op=\"sum\",\n",
    "            input_var=\"volume\",\n",
    "            timeargs=timeargs_vol,\n",
    "            missing=\"pct\",\n",
    "            missing_options={\"tolerance\": 0.15},\n",
    "            interpolate_na=True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "ds_4fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = xd.Query(\n",
    "    **{\n",
    "        \"datasets\": {\n",
    "            \"deh_polygons\": {\n",
    "                \"id\": [\"02*\"],\n",
    "                \"regulated\": [\"Natural\"],\n",
    "                \"variables\": [\"streamflow\"],\n",
    "            }\n",
    "        },\n",
    "        \"time\": {\"start\": \"1970-01-01\", \"minimum_duration\": (15 * 365, \"d\")},\n",
    "    }\n",
    ").data.reset_index()\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a regional analysis, we'll need some explainatory variables. \n",
    "So with those catchments, we can now calculate some of the catchments properties.\n",
    "We could also get meteorological values and land use data. Refer to GIS example for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dswp = xhgis.watershed_properties(\n",
    "    gdf[[\"Station\", \"geometry\"]], unique_id=\"Station\", output_format=\"xarray\"\n",
    ")\n",
    "cent = dswp[\"centroid\"].to_numpy()\n",
    "lon = [ele[0] for ele in cent]\n",
    "lat = [ele[1] for ele in cent]\n",
    "dswp = dswp.assign(lon=(\"Station\", lon))\n",
    "dswp = dswp.assign(lat=(\"Station\", lat))\n",
    "dswp = dswp.drop(\"centroid\")\n",
    "dswp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do our regional frequency analysis, we'll process the data with a principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, pca = xhfa.regional.fit_pca(dswp, n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the correlation is close to 0 between the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_dataframe(name=\"value\").reset_index().pivot(\n",
    "    index=\"Station\", columns=\"components\"\n",
    ").corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different clustering methods can be used. Their parameters can be passed as a dict.\n",
    "We're using AgglomerativeClustering here but later we'll show and exammple using a combination of different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhfa.regional.get_group_from_fit(HDBSCAN, {\"min_cluster_size\": 2}, data)\n",
    "xhfa.regional.get_group_from_fit(OPTICS, {\"min_samples\": 2}, data)\n",
    "groups = xhfa.regional.get_group_from_fit(\n",
    "    AgglomerativeClustering, {\"n_clusters\": 3}, data\n",
    ")\n",
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the L-moments for each station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_moment = xr.apply_ufunc(\n",
    "    moment_l_vector, ds_4fa, input_core_dims=[[\"time\"]], output_core_dims=[[\"lmom\"]]\n",
    ").assign_coords(lmom=[\"l1\", \"l2\", \"l3\", \"tau\", \"tau3\", \"tau4\"])\n",
    "ds_moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create groupes of values and moments for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_groups = xr.concat(\n",
    "    [\n",
    "        ds_4fa.sel(id=groups[i]).assign_coords(group_id=i).expand_dims(\"group_id\")\n",
    "        for i in range(len(groups))\n",
    "    ],\n",
    "    dim=\"group_id\",\n",
    ")\n",
    "ds_moments_groups = xr.concat(\n",
    "    [\n",
    "        ds_moment.sel(id=groups[i]).assign_coords(group_id=i).expand_dims(\"group_id\")\n",
    "        for i in range(len(groups))\n",
    "    ],\n",
    "    dim=\"group_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each group, calculate the H and Z values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kap = KappaGen()\n",
    "ds_H_Z = calc_h_z(ds_groups, ds_moments_groups, kap)\n",
    "ds_H_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the data to only include the data that has H and Z below the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask_h_z(ds_H_Z)\n",
    "ds_groups_H1 = ds_groups.where(mask).load()\n",
    "ds_moments_groups_H1 = ds_moments_groups.where(mask).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centiles and return periods :\n",
    "centiles = [x / 100.0 for x in range(101)]\n",
    "return_periods = [\n",
    "    1.010101,\n",
    "    1.052632,\n",
    "    1.111111,\n",
    "    1.25,\n",
    "    1.5,\n",
    "    2,\n",
    "    3,\n",
    "    5,\n",
    "    10,\n",
    "    20,\n",
    "    50,\n",
    "    100,\n",
    "    200,\n",
    "    500,\n",
    "    1000,\n",
    "    2000,\n",
    "    5000,\n",
    "    10000,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the values for each group and return period and we remove the regions with less stations than a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_T = calculate_rp_from_afr(ds_groups_H1, ds_moments_groups_H1, return_periods)\n",
    "Q_T = remove_small_regions(Q_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot, let see what it looks like on 023401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_reg = Q_T.sel(id=\"023401\").dropna(dim=\"group_id\", how=\"all\")\n",
    "reg = Q_reg.streamflow_max_annual.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare local and regional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_loc = xhfa.local.fit(ds_4fa)\n",
    "Q_loc = xhfa.local.parametric_quantiles(params_loc, return_periods)\n",
    "loc = Q_loc.sel(id=\"023401\", scipy_dist=\"genextreme\").streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Q_reg\n",
    "plt.plot(reg.return_period.values, reg.values, \"blue\")\n",
    "plt.plot(loc.return_period.values, loc.values, \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some uncertainities\n",
    "But we will work with only one catchemnt and two distributions as uncertinities can be intensive in computation.\n",
    "We selct the station 023401, and distribution 'genextreme' and 'pearson3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_4fa_one_station = ds_4fa.sel(id=\"023401\")\n",
    "params_loc_one_station = params_loc.sel(\n",
    "    id=\"023401\", scipy_dist=[\"genextreme\", \"pearson3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bootstrap the observations 200 times to get the uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_4fa_iter = xhfa.uncertainities.boostrap_obs(ds_4fa_one_station, 200)\n",
    "params_boot_obs = xhfa.local.fit(ds_4fa_iter, distributions=[\"genextreme\", \"pearson3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_boot_obs = xhfa.local.parametric_quantiles(\n",
    "    params_boot_obs.load(), return_periods\n",
    ").squeeze()\n",
    "Q_boot_obs = Q_boot_obs.streamflow_max_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, instead of ressampling the observations, we ressamplee the fittted distributions 200 times to get the uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = xhfa.uncertainities.boostrap_dist(\n",
    "    ds_4fa_one_station, params_loc_one_station, 200\n",
    ")\n",
    "params_boot_dist = xhfa.uncertainities.fit_boot_dist(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_boot_dist = xhfa.local.parametric_quantiles(\n",
    "    params_boot_dist.load(), return_periods\n",
    ").squeeze()\n",
    "Q_boot_dist = Q_boot_dist.streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dist = Q_boot_dist.sel(scipy_dist=\"genextreme\")\n",
    "loc_obs = Q_boot_obs.sel(scipy_dist=\"genextreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(reg.return_period.values, reg.values, \"blue\", label=\"Regional\")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.5, \"samples\"),\n",
    "    \"red\",\n",
    "    label=\"bootstrap obs\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.05, \"samples\"),\n",
    "    \"pink\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_obs.return_period.values, loc_obs.quantile(0.95, \"samples\"), \"pink\")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.5, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"bootstrap dist\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.05, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_dist.return_period.values, loc_dist.quantile(0.95, \"samples\"), \"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regional, we need to ressample al stations, but this time, it's much faster as no fit is involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_reg_samples = xhfa.uncertainities.boostrap_obs(ds_4fa, 200)\n",
    "ds_moments_iter = xhfa.uncertainities.calc_moments_iter(ds_reg_samples).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_reg_boot = xhfa.uncertainities.calc_q_iter(\n",
    "    \"023401\", \"streamflow_max_annual\", ds_groups_H1, ds_moments_iter, return_periods\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_boot = Q_reg_boot.streamflow_max_annual.sel(id=\"023401\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(visible=True)\n",
    "plt.plot(\n",
    "    reg_boot.return_period.values,\n",
    "    reg_boot.quantile(0.5, \"samples\"),\n",
    "    \"blue\",\n",
    "    label=\"Regional\",\n",
    ")\n",
    "plt.plot(\n",
    "    reg_boot.return_period.values,\n",
    "    reg_boot.quantile(0.05, \"samples\"),\n",
    "    \"cyan\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(reg_boot.return_period.values, reg_boot.quantile(0.95, \"samples\"), \"cyan\")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.5, \"samples\"),\n",
    "    \"red\",\n",
    "    label=\"bootstrap obs\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.05, \"samples\"),\n",
    "    \"pink\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_obs.return_period.values, loc_obs.quantile(0.95, \"samples\"), \"pink\")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.5, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"bootstrap dist\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.05, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_dist.return_period.values, loc_dist.quantile(0.95, \"samples\"), \"green\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we try different clustering methods\n",
    "We dont do too many tests here since it can take quite a while to run and the goal is just to illustrate the possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM = {\n",
    "    AgglomerativeClustering: {\"arg_name\": \"n_clusters\", \"range\": range(2, 12)},\n",
    "    HDBSCAN: {\"arg_name\": \"min_cluster_size\", \"range\": range(6, 7)},\n",
    "    OPTICS: {\"arg_name\": \"min_samples\", \"range\": range(4, 5)},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our station instead of beein in one region, will be in many of the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations_list = xhfa.uncertainities.generate_combinations(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "\n",
    "for model in [AgglomerativeClustering, HDBSCAN, OPTICS]:\n",
    "\n",
    "    for p in PARAM[model][\"range\"]:\n",
    "        d_param = {}\n",
    "        d_param[PARAM[model][\"arg_name\"]] = p\n",
    "        for combination in combinations_list:\n",
    "            # Extract data for the current combination\n",
    "            data_com = data.sel(Station=list(combination))\n",
    "            # Get groups from the fit and add to the list\n",
    "            groups = groups + get_group_from_fit(model, d_param, data_com)\n",
    "unique_groups = [list(x) for x in {tuple(x) for x in groups}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followin steps are similar to the previous one, just with more regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_groups = xr.concat(\n",
    "    [\n",
    "        ds_4fa.sel(id=unique_groups[i])\n",
    "        .assign_coords(group_id=i)\n",
    "        .expand_dims(\"group_id\")\n",
    "        for i in range(len(unique_groups))\n",
    "    ],\n",
    "    dim=\"group_id\",\n",
    ")\n",
    "ds_moments_groups = xr.concat(\n",
    "    [\n",
    "        ds_moment.sel(id=unique_groups[i])\n",
    "        .assign_coords(group_id=i)\n",
    "        .expand_dims(\"group_id\")\n",
    "        for i in range(len(unique_groups))\n",
    "    ],\n",
    "    dim=\"group_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kap = KappaGen()\n",
    "ds_H_Z = calc_h_z(ds_groups, ds_moments_groups, kap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask_h_z(ds_H_Z)\n",
    "ds_groups_H1 = ds_groups.where(mask).load()\n",
    "ds_moments_groups_H1 = ds_moments_groups.where(mask).load()\n",
    "\n",
    "Q_T = calculate_rp_from_afr(ds_groups_H1, ds_moments_groups_H1, return_periods)\n",
    "Q_T = remove_small_regions(Q_T)\n",
    "\n",
    "Q = Q_T.sel(id=\"023401\").dropna(dim=\"group_id\", how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_multiple_region = Q.streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_moment = xr.apply_ufunc(\n",
    "    moment_l_vector, ds_4fa, input_core_dims=[[\"time\"]], output_core_dims=[[\"lmom\"]]\n",
    ").assign_coords(lmom=[\"l1\", \"l2\", \"l3\", \"tau\", \"tau3\", \"tau4\"])\n",
    "ds_moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.plot(\n",
    "    regional_multiple_region.return_period.values,\n",
    "    regional_multiple_region.quantile(0.5, \"group_id\"),\n",
    "    \"blue\",\n",
    "    label=\"regional_multiple_region\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region.return_period.values,\n",
    "    regional_multiple_region.quantile(0.05, \"group_id\"),\n",
    "    \"cyan\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region.return_period.values,\n",
    "    regional_multiple_region.quantile(0.95, \"group_id\"),\n",
    "    \"cyan\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.5, \"samples\"),\n",
    "    \"red\",\n",
    "    label=\"bootstrap obs\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.05, \"samples\"),\n",
    "    \"pink\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_obs.return_period.values, loc_obs.quantile(0.95, \"samples\"), \"pink\")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.5, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"bootstrap dist\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.05, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_dist.return_period.values, loc_dist.quantile(0.95, \"samples\"), \"green\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(visible=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also combine multiple regions and ressampling.\n",
    "calc_q_iter will check in how many group_id the station is present, and stack it with samples\n",
    "In this case, it will be stacked with 200 samples, and it's in 533 groupes so 103600 samples are generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_reg_boot = xhfa.uncertainities.calc_q_iter(\n",
    "    \"023401\", \"streamflow_max_annual\", ds_groups_H1, ds_moments_iter, return_periods\n",
    ")\n",
    "Q_reg_boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_multiple_region_boot = Q_reg_boot.sel(id=\"023401\").streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.5, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"bootstrap dist\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.05, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_dist.return_period.values, loc_dist.quantile(0.95, \"samples\"), \"green\")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.5, \"samples\"),\n",
    "    \"red\",\n",
    "    label=\"bootstrap obs\",\n",
    ")\n",
    "plt.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.05, \"samples\"),\n",
    "    \"pink\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(loc_obs.return_period.values, loc_obs.quantile(0.95, \"samples\"), \"pink\")\n",
    "plt.plot(\n",
    "    regional_multiple_region_boot.return_period.values,\n",
    "    regional_multiple_region_boot.quantile(0.5, \"samples\"),\n",
    "    \"black\",\n",
    "    label=\"regional multiple regions and boot\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region_boot.return_period.values,\n",
    "    regional_multiple_region_boot.quantile(0.05, \"samples\"),\n",
    "    \"grey\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region_boot.return_period.values,\n",
    "    regional_multiple_region_boot.quantile(0.95, \"samples\"),\n",
    "    \"grey\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region.return_period.values,\n",
    "    regional_multiple_region.quantile(0.5, \"group_id\"),\n",
    "    \"blue\",\n",
    "    label=\"regional multiple regions\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region.return_period.values,\n",
    "    regional_multiple_region.quantile(0.05, \"group_id\"),\n",
    "    \"cyan\",\n",
    "    label=\"95% CI\",\n",
    ")\n",
    "plt.plot(\n",
    "    regional_multiple_region.return_period.values,\n",
    "    regional_multiple_region.quantile(0.95, \"group_id\"),\n",
    "    \"cyan\",\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(visible=True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
